下面我把你提供的 **DR Agent 论文（Deep Research Agents: A Systematic Examination And Roadmap）** 的核心框架（参考架构 + taxonomy + 路线图思想），和我们对话里讨论过的 **LangGraph（状态机/有向图编排）**、以及 **Anthropic 工程化思路（long‑running harness / context engineering / 多智能体研究系统 / postmortem / 权限与安全）** 结合起来，做一次“**把研究型智能体的方法论迁移到编程多智能体框架**”的深入落地推演。

> 目标不是“再列一堆角色名”，而是回答：  
> **如何把 DR agent 的系统观（数据面/控制面、混合检索、DAG 并行、外置记忆、验证闭环）真正变成一个能稳定做软件工程任务的多智能体框架。**

---

## 0. 先做一个关键映射：DR（研究）≈ SWE（编程）共通的系统形态

DR 论文强调：Deep Research 不是一次问答，是一个**长程、迭代、工具密集、需要可核查产物**的工作流系统。

软件工程任务（SWE）与其高度同构：

| DR agent 任务要素 | SWE 多智能体中的对应物 |
|---|---|
| 多轮检索（网页/API） | 代码库检索（ripgrep/LSP/索引）+ 官方文档/issue/PR/StackOverflow |
| 工具使用（计算/分析） | 运行测试、lint、typecheck、build、benchmark、静态分析、依赖分析 |
| 结构化报告（带引用） | PR（diff）+ 测试证据 + 变更说明 + 设计决策记录 |
| 事实核查/交叉验证 | 通过 CI、可复现用例、回归测试、对比基线性能 |
| 记忆/压缩/外部存储 | repo 索引、symbol 图、变更日志、失败案例库、轨迹（trace） |
| 并行与调度（DAG） | 并行跑多测试套件、并行实现/写测试/写文档/做安全审查 |
| 规划策略（澄清→计划→执行） | 需求澄清→技术方案→分解任务→实现→验证→迭代 |

所以把 DR 的“系统工程”搬到编程域，关键不是“让模型会写代码”，而是构建一个能**持续迭代、可回放、可验证、可并行**的框架（这正是 LangGraph + harness 的强项）。

---

## 1. 用 DR 论文的 taxonomy 反推：编程多智能体应选什么工作流形态？

DR taxonomy 里最有用的三个维度：  
**静态 vs 动态工作流、规划策略、单体 vs 多体**。迁移到 SWE，我建议一个“半静态主干 + 动态支路”的折中。

### 1.1 静态 vs 动态：软件工程最适合“主干静态，分支动态”
- **静态主干（强约束、可审计、可控）**：  
  需求澄清 → 方案/任务分解 → 实现 → 测试/验证 → 产出 PR/报告  
  这条主干最好固定，因为它对应团队工程规范、CI 结构、审查流程。
- **动态支路（模型自主探索、可回溯）**：  
  - “查不到原因 → 扩大检索范围”  
  - “测试失败 → 自动定位/最小复现/回滚部分变更 → 重规划”  
  - “依赖冲突 → 选择替代实现或升级策略”  
  这些地方需要动态 replanning，否则会非常僵硬。

**DR 启示**：静态提高运行稳定性，动态提高任务覆盖与泛化；SWE 里两者都要。

---

### 1.2 规划策略：更推荐 Unified intent‑planning（先给计划让人确认）
DR 论文指出：先澄清/先对齐计划能显著减少无效检索和返工。SWE 里“返工”更贵，因为会带来大量 diff、引入新 bug、CI 成本高。

**推荐交互策略（非常工程化）**：
1) 先问 3–7 个澄清问题（输入不足时）  
2) 给出一个**可执行计划**（含：要改哪些文件/模块、要加哪些测试、验收标准、风险点）  
3) 用户确认后再动代码

这相当于把“算力成本控制机制”前置，也让后续多智能体协作有共同目标。

---

### 1.3 单智能体 vs 多智能体：SWE 更需要“动态多智能体 + 强调共享工件”
DR 论文提到多智能体的优势在并行与专精，但协同复杂。SWE 的协同复杂度更高（代码必须一致、接口契约必须匹配、测试必须通过），因此**多智能体必须围绕共享工件（artifacts）协作**，而不是围绕聊天协作。

你应该把协作中心从“对话消息”迁移为：
- `设计决策记录（ADR）`
- `任务 DAG`
- `diff/patch`
- `测试结果与日志`
- `代码索引与证据片段（代码片段引用）`

这就是 DR 论文“结构化报告/证据”的 SWE 版落地。

---

## 2. DR 参考架构迁移：把“研究闭环”改造成“编码闭环”

DR 参考架构主链路：  
**输入 → 澄清/计划 → 多轮检索 → 多轮工具 → 记忆压缩 → 核查回溯 → 结构化输出**

迁移到编程工作，变为：

**需求输入 → 澄清/验收标准 → 技术方案/任务分解（DAG） →（并行）代码库检索/外部文档检索 → 实现（生成 patch）→ 运行工具链（test/lint/typecheck/build）→ 失败诊断与回溯（replan）→ 输出 PR（diff + 证据 + 说明）**

你会发现核心变化是：  
- DR 的“证据”来自网页/论文；SWE 的“证据”更多来自 **测试/构建结果** 与 **对代码库事实的引用**（哪些函数、哪些契约、哪些调用链）。
- DR 的“报告”是文字；SWE 的“报告”是 **可合并的变更集** + 可审计说明。

---

## 3. 架构上最关键的一刀：分离“控制面（Control Plane）”与“数据面（Data Plane）”

这是你提供文档里我认为最重要、且最能提升编程 agent 可靠性的工程哲学之一：

### 3.1 数据面（Data Plane）：负责“拿到事实、产生可验证结果”
在 SWE 里数据面包括：
- 代码库读取：文件系统、git、diff
- 结构化索引：LSP、ctags、ripgrep、AST/依赖图
- 外部信息：官方文档、issue、release note
- 工具链输出：测试日志、覆盖率、benchmark、静态分析报告

数据面要尽量做到：**可复现、可缓存、可引用、低延迟优先**。

### 3.2 控制面（Control Plane）：负责“决定下一步做什么、何时停止、何时回溯”
控制面包括：
- 任务分解（DAG）
- 调度（并行/优先级/预算）
- 工具选择策略（先跑哪些测试、先 lint 还是先 build）
- 失败处理策略（重试/降级/请求人工）
- 安全策略（哪些操作需要审批）

> 把控制面做成显式系统逻辑（图/状态机），而不是塞进 prompt 里，是从“能跑”到“可维护”的分水岭。

这也正是 LangGraph 的核心价值：把控制面“外显”。

---

## 4. 用 LangGraph 把这些“系统观”落到可运行的多智能体框架

你在上一轮问 LangGraph，我给过“状态图/节点/条件边/循环/持久化”的解释。现在把它具体化为 SWE 多智能体框架的骨架。

### 4.1 建议的全局 State（这是成败关键）
不要只存 messages。建议至少包含：

- `goal`: 用户目标/需求原文
- `acceptance_criteria`: 验收标准（结构化 checklist）
- `repo_meta`: repo 结构、语言、构建方式、入口
- `task_dag`: 子任务、依赖关系、状态（todo/doing/done/blocked）
- `artifacts`:
  - `patches`（每次变更的 diff）
  - `design_notes`（ADR）
  - `code_refs`（引用过的关键文件/符号）
- `evidence`（SWE 版证据）:
  - `test_results`（命令、退出码、失败用例、日志摘要）
  - `lint_results` / `typecheck_results` / `build_results`
  - `benchmarks`（若性能相关）
- `retrieval`:
  - `local_search_hits`（ripgrep 命中摘要）
  - `external_docs`（来源 + 摘要）
- `budget`:
  - token、工具调用次数、最长 wall time
- `risk_flags`:
  - 是否涉及安全敏感（鉴权、加密、支付）
  - 是否涉及大范围重构
- `audit_log` / `decision_log`（用于 postmortem）

这完全对应你提供文档里强调的：**外置记忆 + 结构化存储 + 可回溯**。

---

### 4.2 建议的 Agent 角色设计（围绕工件，不围绕“人格”）
典型 SWE 多智能体可以这样分工（每个 agent 最好输出结构化产物）：

1) **Supervisor / Project Manager（控制面核心）**
   - 产物：任务 DAG、优先级、预算分配、何时停止/回溯
2) **Repo Scout（代码库侦察/检索）**
   - 产物：关键路径图、相关文件清单、符号引用证据
3) **Architect（方案与接口设计）**
   - 产物：ADR（决策、替代方案、风险）
4) **Implementer（实现者，可多个并行）**
   - 产物：patch（diff），局部单元测试
5) **Test/CI Agent（验证者）**
   - 产物：测试命令、失败定位、最小复现步骤
6) **Code Reviewer / Critic（审查者）**
   - 产物：问题清单（正确性、边界条件、风格、性能、安全）
7) **Doc/Release Agent（交付包装）**
   - 产物：变更说明、迁移指南、PR 描述、Changelog

**DR 启示的关键点**：  
多智能体价值来自并行与专精，但必须靠 **共享记忆与结构化工件** 保持一致性。

---

### 4.3 控制流：用“DAG + 循环回溯”的图，而不是线性流水线

你可以在 LangGraph 里用“主干 + 动态回路”实现：

**主干（相对静态）**
1. `clarify_intent`（澄清+验收标准）
2. `make_plan_dag`（生成任务 DAG）
3. `repo_scout_parallel`（并行：符号索引/全局搜索/相关模块扫描）
4. `design_adr`（方案）
5. `implement_parallel`（并行多个 implementer 按 DAG 子任务产出 patch）
6. `run_verification`（test/lint/typecheck/build）
7. `review`（critic）
8. `finalize_pr`（PR 包装）

**动态回路（关键！）**
- 如果 `verification` 失败 → `debug_and_replan` → 回到 `implement` 或 `repo_scout`
- 如果 `review` 指出风险 → 回到 `design` 或 `implement`
- 如果发现需求不清 → 回到 `clarify_intent`

这就是 DR 架构里的“迭代闭环”在 SWE 中的等价实现。

---

## 5. 检索（Retrieval）迁移：从“网页检索”变成“代码库检索 + 文档检索”的混合分层

DR 论文强调 API vs 浏览器两层；在 SWE 里可以对应为：

### 5.1 第一层（最便宜、最高确定性）：本地结构化检索
优先顺序建议：
1) LSP/索引（找符号定义、引用、类型信息）
2) ripgrep / 搜索（字符串、函数名、错误信息）
3) AST/依赖图（调用链、模块依赖）
4) git 历史（最近相关 commit、相关 PR）

这些都属于“API-like retrieval”：快、稳、可缓存、可重复。

### 5.2 第二层：官方文档/依赖库资料（仍尽量 API 化）
- 优先：官方 docs（或本地 vendor docs）、README、CHANGELOG、issue 模板
- 再考虑：网页浏览器（作为兜底）

### 5.3 第三层：浏览器（最后兜底）
当遇到：
- JS 渲染文档
- 需要登录/交互
- 需要下载 PDF/长文
才用浏览器自动化。

> **迁移原则**：  
> 像 DR 一样，做“混合检索”：能结构化就结构化，浏览器是昂贵兜底。

---

## 6. 工具使用（Tool Use）迁移：把“事实核查”变成“可复现验证”

DR 里核查是交叉引用来源；SWE 里核查应围绕“可复现的验证工具链”。

### 6.1 必备工具集（最小闭环）
- `apply_patch` / `git` 操作（创建分支、提交）
- `run_tests`（可配置：fast tests / full suite）
- `lint` / `format`  
- `typecheck`（TS/mypy/clang-tidy 等）
- `build`（确保可编译/可打包）
- `run_example`（最小 demo/CLI 冒烟）

### 6.2 Tool-Integrated Reasoning（TIR）的 SWE 落地
DR 论文提到要优化“工具选择与时机”。SWE 中你可以把工具策略写成**显式调度规则**（后续再学习化）：

- 改动范围小：先跑最相关的测试子集（按路径映射/标签）
- 改动核心库：必须跑全套 + 性能回归（如有）
- 引入新依赖：必须跑安全扫描/许可证检查（如组织要求）

这类策略放在**控制面**（图/节点）里，比“让模型自己决定跑不跑测试”可靠一个数量级。

---

## 7. 记忆（Memory）迁移：把“长上下文”替换为“外置工件 + 摘要 + 可回放轨迹”

DR 论文强调：长上下文不是万能，外置 + 压缩 + 结构化更关键。SWE 更是如此，因为代码库太大、迭代太多。

### 7.1 短期记忆：面向当前回合的“上下文打包”
每个节点只拿它需要的上下文（context engineering）：
- 当前要改的文件片段
- 接口契约/类型定义
- 失败测试的日志摘要
- ADR 决策

不要把整个 repo/所有历史对话塞进 prompt。

### 7.2 长期记忆：面向复用的“案例库（CBR）+ 符号知识”
强烈建议建立：
- **Fix-case bank**：错误模式 → 修复策略 → 相关 commit 的简述  
  例如：某类 TypeScript 类型收窄错误如何修、某类 Python packaging 问题如何处理
- **Symbol graph / Knowledge graph**（轻量即可）：
  - 关键模块、职责、依赖方向、禁止依赖规则
- **Trace store**：每次运行的状态快照、工具调用日志、关键决策

这对应 DR 文档里提到的“非参数学习（CBR）”路径：不一定更新模型参数，也能让系统越来越会做事。

---

## 8. 并行与调度：把“研究任务 DAG”升级为“工程任务 DAG”

DR 论文强调异步并行与 DAG。SWE 里最自然的 DAG 是：

- 节点：子任务（实现 A、实现 B、写测试、更新文档、迁移脚本…）
- 边：依赖（先改接口再改调用方、先加测试再修 bug 等）
- 并行收益：实现与测试、不同模块改动、不同平台 CI 都可并行

### 8.1 一个实用的 DAG 拆分模板
当用户说“实现一个功能/修一个 bug”，Supervisor 先生成 DAG：

1) **理解与定位**
   - 找入口、相关模块、现有行为、失败用例
2) **契约定义**
   - API/函数签名/行为变更、兼容性策略
3) **实现**
   - 子任务按模块分配给多个 implementer
4) **测试**
   - 单元测试、集成测试、回归测试
5) **文档与迁移**
6) **验证与审查**
   - CI、性能、安全

### 8.2 调度策略（从规则到学习）
MVP 阶段建议用规则调度：
- 优先解决“阻塞性失败”（build/test 失败）
- 优先合并共享依赖的任务（减少冲突）
- 资源受限时限制并行度（避免上下文/工具争用）

后续你可以引入 DR 论文提到的“学习型调度器”思想：把任务时延、失败率、信息增益作为奖励信号，但那是 v2/v3 的事。

---

## 9. “安全与权限”迁移：从 DR 的“权限提示”走向 SWE 的“策略化变更治理”

你在对话历史里提到 Anthropic 相关工程主题（不靠弹窗权限提示）。在 SWE 框架里，最重要的是把高风险动作做成显式分支：

- 允许自动执行：读文件、搜索、跑测试、格式化
- 需要审批：大范围重构、升级主版本依赖、修改鉴权/加密/支付、删除大量文件、写入生产配置
- 禁止自动执行：推送到受保护分支、发布到生产（除非你有完备流程）

在 LangGraph 里就是一个 `policy_gate_node`：
- 输入：risk_flags + diff 摘要 + 影响面分析
- 输出：ALLOW / REQUIRE_REVIEW / DENY，并写入 audit_log

这比“模型自觉小心”更可靠，也更符合企业工程实践。

---

## 10. 质量闭环（验证/审查）是“总效率”的来源：少返工才是真高效

DR 文档强调：可靠性看似增加步骤，实则减少下游返工。SWE 里尤为明显。

### 10.1 建议做成两个“质量门”
1) **Pre-commit gate（实现后、提交前）**
   - fast tests + lint + typecheck
2) **Pre-merge gate（PR 级别）**
   - full suite + 安全扫描（如需要）+ 兼容性检查

### 10.2 Critic 不是“挑刺人格”，而是结构化验收器
让 reviewer/critic 输出这种结构化结果：
- 是否满足每条验收标准（逐条 pass/fail）
- 是否引入 breaking change
- 是否有未覆盖边界条件
- 是否有性能/安全风险
- 建议补充的测试列表

然后控制面根据这些结果决定是否回环。

---

## 11. 一个可直接照抄的“编程多智能体框架蓝图”（从 MVP 到可用）

### 11.1 MVP（两周内能跑）
- 单 repo
- 主干静态 + 失败回路
- 角色：Supervisor + RepoScout + Implementer + Tester + Reviewer
- 工具：搜索、读写文件、跑测试、lint/format
- 输出：可合并 patch + 测试证据 + 说明

### 11.2 v1（开始“像系统”）
- DAG 子任务并行（至少并行：检索/测试/实现）
- 外置记忆：符号索引 + 失败案例库
- Checkpoint/回放：每次工具调用与关键决策存档
- 策略节点：预算/风险/停止条件

### 11.3 v2（追求规模化与团队协作）
- MCP 化工具接入（统一协议/权限/审计）
- 多 repo / monorepo 支持
- 学习化调度（可选）
- 与 CI/CD 深度集成：自动开 PR、自动更新说明、自动贴证据

这条路线与 DR 论文的 roadmap 思维一致：先把系统结构搭对，再逐步把策略优化/学习化。

---

## 12. 具体到 LangGraph：我建议的“节点清单 + 条件边”设计（可当实现说明书）

下面是一套很实用的节点设计（名称可换）：

1) `ingest_request`：解析用户目标、约束
2) `clarify_and_acceptance`：生成澄清问题；输出验收标准
3) `plan_dag`：生成任务 DAG（结构化）
4) `repo_index_and_search`（可并行）：
   - `symbol_index`
   - `ripgrep_search`
   - `git_history_search`
5) `design_adr`：方案与风险
6) `policy_gate`：是否允许自动改代码/改哪些范围
7) `implement_tasks_parallel`：按 DAG 分配实现
8) `apply_patch_and_commit`：落盘变更（可加 checkpoint）
9) `run_fast_verification`：fast tests + lint + typecheck
10) `debug_or_replan`（条件）：
    - 若失败：定位 → 最小复现 → 修复策略 → 回到 implement
11) `run_full_verification`：全量测试/构建
12) `review_and_checklist`：critic 输出验收表
13) `finalize_pr_bundle`：PR 描述、变更说明、证据汇总
14) `human_review_optional`：需要人审时停住等待
15) `end`

**条件边示例**：
- `if unclear_requirements -> clarify_and_acceptance`
- `if risky_changes -> human_review_optional`
- `if tests_fail -> debug_or_replan`
- `if acceptance_fail -> implement_tasks_parallel`
- `if budget_exceeded -> stop_with_report`

这就是把 DR 的“闭环、回溯、验证、结构化产物”完全编程化。

---

## 13. 为了让方案更贴近你的实际：我需要你回答 5 个问题（决定架构细节）

你希望这个“编程多智能体框架”服务于哪种工作形态？不同场景取舍会差很多：

1) 目标更偏：**修 bug**、**加功能**、**重构**、**写测试**、还是 **代码审查/安全审计**？  
2) 主要语言/生态是什么（Python/JS/TS/Java/Go/Rust…）？  
3) 是否允许访问外网文档？是否有内网依赖（私有包、内部 wiki）？  
4) 交付物是：直接改 main、开 PR、还是输出 patch 让人手动应用？  
5) 你更看重：速度（尽快出结果）还是可靠性（CI 全绿、证据齐全、少返工）？

你给出这些信息后，我可以把上面的蓝图进一步具体化为：
- **一个明确的 LangGraph 状态结构（TypedDict）**
- **任务 DAG 生成规则**
- **每个 agent 的输入/输出契约（A2A 消息/工件格式）**
- **最小工具集（MCP 接入建议、权限策略）**
- **一条可执行的实现里程碑（MVP→v1→v2）**